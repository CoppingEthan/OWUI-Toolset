# Fix: LLM Cannot See Uploaded Files + Hallucination on Large Messages

## Issue Summary

When users uploaded files in Open WebUI, the LLM either couldn't see the file content, hallucinated completely unrelated responses, or told users "your message didn't come through." This was a fatal bug affecting all file uploads where the extracted content exceeded 8,192 tokens.

## Root Cause

Open WebUI (OWUI) embeds CEE-extracted file content directly into the user's message text before sending it to the server. The oversized message replacement logic in `src/api/server.js` (lines 912-935) then **replaced the entire message** — including the user's actual question — with a generic notice:

```
[This message exceeded the maximum allowed length of 8,192 tokens and was removed]
```

The LLM received only this notice, had zero context about what the user asked, and either:
- Hallucinated responses based on user memories / system prompt
- Fell back to the default greeting
- Told the user "your message didn't come through"

The replacement logic treated ALL user messages identically — it did not distinguish the current message (which the user just sent) from older messages in the conversation history.

## Old Behaviour

- **Any** user message exceeding ~26,214 characters (8,192 tokens at 3.2 chars/token) was fully replaced with a removal notice
- This applied to the **latest** user message as well as older messages
- CEE-extracted file content had **no size cap** — a 100-page PDF could generate megabytes of markdown
- The LLM received no file content, no user question, just `[message removed]`

## New Behaviour

### Layer 1: CEE File Content Truncation (`/process` endpoint)
- Each file's extracted content is now capped at `MAX_USER_MESSAGE_TOKENS` (~8k tokens) before being returned to OWUI
- Truncation preserves the **first half** and **last half** of the content with a notice in the middle
- The notice includes the sandbox file path so the LLM can agentically read the full file

### Layer 2: Smart Chat Message Truncation (chat endpoint)
- **Last user message (current turn):** Truncated using first-half + notice + last-half instead of full replacement. The limit dynamically scales: `8k x (1 + fileCount)`, so each attached file gets its own 8k budget on top of the base 8k for user text.
- **Older user messages (history):** Still fully replaced with a notice to keep conversation history lean (unchanged behaviour for old messages).

### Token Budget Examples
| Scenario | Last Message Limit |
|----------|-------------------|
| No files | 8k tokens |
| 1 file | 16k tokens (8k user + 8k file) |
| 3 files | 32k tokens (8k user + 3x8k files) |

## Files Changed

### `src/api/server.js`

**CEE truncation (after line 328, new lines 330-343):**
Added truncation of `page_content` after `extractContent()` returns and before responding to OWUI. Uses `MAX_USER_MESSAGE_TOKENS` as the per-file budget. Preserves first/last halves with a `✂️` truncation notice containing the sandbox path.

**Chat endpoint message handling (was lines 912-935, now lines 927-1007):**
Replaced the flat `for (const msg of processedMessages)` loop with an indexed loop that:
1. Finds the last user message index (`lastUserIdx`)
2. Applies dynamic limit for the latest message vs fixed limit for older messages
3. Uses first-half + notice + last-half truncation for the latest message
4. Handles both string content and multimodal (array) content with image block preservation

**JSDoc comment (was lines 1410-1414, now lines 1482-1490):**
Updated the `MAX_USER_MESSAGE_TOKENS` constant documentation to describe the new truncation-instead-of-replacement behaviour and dynamic file scaling.

### `.env.example` (lines 46-50)
Added documentation for `MAX_USER_MESSAGE_TOKENS` under the Limits section.

### `README.md` (configuration table)
Added `MAX_USER_MESSAGE_TOKENS` row to the configuration variable table.

## Key Design Decisions

1. **First-half + last-half truncation:** User questions typically appear at the start of the message, file content follows. The end of a document often contains conclusions, signatures, or summaries. Preserving both ends gives the LLM the best chance of understanding context.

2. **Dynamic limit scaling with file count:** The `files` array in the request body tells us how many files are attached to the current turn. Each file gets its own 8k budget so uploading multiple files doesn't cause them to compete for space.

3. **CEE truncation is the primary control:** By capping file content at the `/process` endpoint (before OWUI gets it), we prevent massive content from ever entering the message. The chat endpoint truncation is a safety net.

4. **Sandbox path in truncation notice:** Telling the LLM the file is at `/workspace/uploaded/filename` enables agentic follow-up — the LLM can use `sandbox_read_file` to access sections of the full document.

5. **`MAX_USER_MESSAGE_TOKENS=0` disables both truncations:** Consistent with the existing convention where 0 means unlimited.
